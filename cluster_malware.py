import os
import json
from pathlib import Path
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import normalize
import torch
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_samples, silhouette_score
from datetime import datetime
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist
from scipy.spatial.distance import pdist
from sklearn.decomposition import PCA
import re


def load_malicious_code(base_path):
    """从指定目录加载包的代码,将每个包内的所有.py文件代码拼接"""
    package_codes = []
    package_paths = []
    
    packages = [d for d in Path(base_path).iterdir() if d.is_dir()]
    
    for package_path in packages:
        package_code = []
        
        for py_file in package_path.rglob("*.py"):
            if py_file.name.startswith('._') or not py_file.is_file():
                continue
                
            try:
                for encoding in ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']:
                    try:
                        with open(py_file, 'r', encoding=encoding) as f:
                            code = f.read()
                            if code.strip() and isinstance(code, str):
                                package_code.append(code)
                                break
                    except UnicodeDecodeError:
                        continue
            except Exception as e:
                continue
        
        if package_code:
            full_code = "\n\n".join(package_code)
            package_codes.append(full_code)
            package_paths.append(str(package_path))
    
    return package_codes, package_paths

def split_code_into_segments(code, max_length=512):
    lines = code.split('\n')
    segments = []
    current_segment = []

    for line in lines:
        current_segment.append(line)
        if len(' '.join(current_segment).split()) > max_length:
            segments.append('\n'.join(current_segment[:-1]))
            current_segment = [current_segment[-1]]

    if current_segment:
        segments.append('\n'.join(current_segment))

    return segments

def attention_based_encoding(code, tokenizer, model, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """使用CodeBERT编码代码"""
    try:
        model = model.to(device)
        inputs = tokenizer(code, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            # 使用最后一层的[CLS]token的输出，而不是平均所有token
            result = outputs.last_hidden_state[:, 0, :]
            return result.cpu().numpy().squeeze()
    except Exception as e:
        return None

def extract_code_features(code):
    """提取代码的基本特征用于过滤"""
    # 检测编码内容
    suspicious_patterns = {
        'hex': r'\\x[0-9a-fA-F]{2}',
        'base64': r'[A-Za-z0-9+/=]{20,}',
        'unicode': r'\\u[0-9a-fA-F]{4}',
    }
    
    total_chars = len(code)
    encoded_chars = 0
    
    for pattern in suspicious_patterns.values():
        matches = re.findall(pattern, code)
        encoded_chars += sum(len(m) for m in matches)
    
    return encoded_chars / total_chars if total_chars > 0 else 0

def attention_based_encoding_segmented(code, tokenizer, model, device='cuda' if torch.cuda.is_available() else 'cpu'):
    segments = split_code_into_segments(code)
    encoded_segments = []
    
    model = model.to(device)
    
    for segment in segments:
        encoding = attention_based_encoding(segment, tokenizer, model, device)
        if encoding is not None:
            encoded_segments.append(encoding)
    
    if encoded_segments:
        return np.mean(encoded_segments, axis=0)
    
    return None

def calculate_cluster_similarity(X_scaled, cluster_labels, cluster_id):
    cluster_samples = X_scaled[cluster_labels == cluster_id]
    if len(cluster_samples) < 2:
        return None
    
    distances = pdist(cluster_samples, metric='euclidean')
    similarity = 1 / (1 + np.mean(distances))
    
    return float(similarity)

def process_and_encode(base_path, output_path, batch_size=32):
    """处理和编码代码"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    
    tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
    model = AutoModel.from_pretrained("microsoft/codebert-base").to(device)
    
    print("Loading malicious code...")
    codes, file_paths = load_malicious_code(base_path)
    
    print("Encoding code samples...")
    encodings = []
    valid_indices = []
    
    for i in tqdm(range(0, len(codes), batch_size)):
        batch_codes = codes[i:i+batch_size]
        batch_encodings = []
        
        for code in batch_codes:
            encoding = attention_based_encoding_segmented(code, tokenizer, model, device)
            if encoding is not None:
                batch_encodings.append(encoding)
                valid_indices.append(i + len(batch_encodings) - 1)
        
        if batch_encodings:
            encodings.extend(batch_encodings)
            
        if i % 1000 == 0 and i > 0:
            temp_output = {
                "processed_count": len(encodings),
                "valid_indices": valid_indices,
                "timestamp": datetime.now().isoformat()
            }
            with open(Path(output_path) / "encoding_progress.json", 'w') as f:
                json.dump(temp_output, f)
    
    return np.vstack(encodings), [file_paths[i] for i in valid_indices], [codes[i] for i in valid_indices]

def find_optimal_clusters(X, max_clusters=500, step=50):
    X_scaled = normalize(X)
    distortions = []
    silhouette_scores = []
    K = range(50, max_clusters, step)
    
    for k in tqdm(K):
        kmeans = KMeans(n_clusters=k, random_state=42).fit(X_scaled)
        distortions.append(sum(np.min(cdist(X_scaled, kmeans.cluster_centers_, 'cosine'), axis=1)) / X_scaled.shape[0])
        if k > 1:
            silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_, metric='cosine'))
    
    diffs = np.diff(distortions)
    elbow_idx = np.argmax(diffs) + 1
    optimal_k = K[elbow_idx]
    
    best_silhouette_idx = np.argmax(silhouette_scores)
    best_silhouette_k = K[best_silhouette_idx]
    
    return int((optimal_k + best_silhouette_k) / 2)

def recluster_large_clusters(X_pca, clusters, threshold=50):
    cluster_sizes = np.bincount(clusters)
    large_clusters = np.where(cluster_sizes > threshold)[0]
    
    if not large_clusters.any():
        return clusters
        
    new_clusters = clusters.copy()
    max_cluster_id = max(clusters)
    
    for large_cluster in large_clusters:
        indices = np.where(clusters == large_cluster)[0]
        samples = X_pca[indices]
        
        n_subclusters = max(2, len(indices) // 20)
        
        sub_kmeans = KMeans(n_clusters=n_subclusters, random_state=42)
        sub_clusters = sub_kmeans.fit_predict(samples)
        
        for i, idx in enumerate(indices):
            new_clusters[idx] = max_cluster_id + 1 + sub_clusters[i]
        max_cluster_id = max(new_clusters)
    
    return new_clusters

def cluster_and_save(X, file_paths, codes, output_path, n_clusters=400):
    X_scaled = normalize(X)
    pca = PCA(n_components=0.95)
    X_pca = pca.fit_transform(X_scaled)
    
    print(f"\nStarting initial K-means clustering with {n_clusters} clusters...")
    initial_kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = initial_kmeans.fit_predict(X_pca)
    
    # 对大簇进行二次聚类
    clusters = recluster_large_clusters(X_pca, clusters, threshold=50)
    
    # 获取最终的簇数
    final_n_clusters = len(np.unique(clusters))
    print(f"\nFinal number of clusters after reclustering: {final_n_clusters}")
    
    # 使用最终的簇数重新进行聚类
    final_kmeans = KMeans(n_clusters=final_n_clusters, random_state=42)
    clusters = final_kmeans.fit_predict(X_pca)
    
    # 计算每个簇的质量指标
    cluster_scores = silhouette_samples(X_pca, clusters, metric='cosine')
    
    valid_clusters = []
    similarities = []
    
    print("Processing clusters...")
    for cluster_id in tqdm(range(final_n_clusters)):
        cluster_indices = np.where(clusters == cluster_id)[0]
        
        # 使用新的kmeans对象计算距离
        center_distances = cdist([final_kmeans.cluster_centers_[cluster_id]], X_pca[cluster_indices], metric='cosine')[0]
        
        cluster_members = []
        for idx, dist in zip(cluster_indices, center_distances):
            member = {
                "file_path": file_paths[idx],
                "code": codes[idx],
                "distance_to_center": float(dist),
                "silhouette_score": float(cluster_scores[idx]),
                "code_length": len(codes[idx])
            }
            cluster_members.append(member)
        
        # 计算簇内相似度
        if len(cluster_indices) > 1:
            pair_similarities = cosine_similarity(X_pca[cluster_indices])
            sim_values = pair_similarities[np.triu_indices(len(pair_similarities), k=1)]
            cluster_similarity = float(np.mean(sim_values))
            similarities.append(cluster_similarity)
        else:
            cluster_similarity = "N/A"
            sim_values = []
        
        cluster_info = {
            "cluster_id": int(cluster_id),
            "size": len(cluster_members),
            "average_similarity": cluster_similarity,
            "average_silhouette": float(np.mean([m["silhouette_score"] for m in cluster_members])),
            "all_members": cluster_members,
            "statistics": {
                "min_similarity": float(np.min(sim_values)) if len(sim_values) > 0 else "N/A",
                "max_similarity": float(np.max(sim_values)) if len(sim_values) > 0 else "N/A",
                "average_code_length": float(np.mean([m["code_length"] for m in cluster_members]))
            }
        }
        
        valid_clusters.append(cluster_info)
    
    # 保存结果
    output_file = Path(output_path) / "malware_clusters1.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(valid_clusters, f, indent=2, ensure_ascii=False)
    
    # 打印统计信息
    multi_sample_clusters = [c for c in valid_clusters if c["size"] > 1]
    single_sample_clusters = [c for c in valid_clusters if c["size"] == 1]
    
    print("\nClustering Summary:")
    print(f"Total samples (after filtering): {sum(c['size'] for c in valid_clusters)}")
    print(f"Total clusters: {len(valid_clusters)}")
    print(f"Multi-sample clusters: {len(multi_sample_clusters)}")
    print(f"Single-sample clusters: {len(single_sample_clusters)}")
    
    cluster_sizes = [c["size"] for c in valid_clusters]
    print(f"\nCluster Size Statistics:")
    print(f"Average cluster size: {np.mean(cluster_sizes):.2f}")
    print(f"Largest cluster size: {max(cluster_sizes)}")
    print(f"Smallest cluster size: {min(cluster_sizes)}")
    
    if similarities:
        print(f"\nSimilarity Statistics (multi-sample clusters):")
        print(f"Average similarity: {np.mean(similarities):.3f}")
        print(f"Min similarity: {np.min(similarities):.3f}")
        print(f"Max similarity: {np.max(similarities):.3f}")
    
    # 添加代码长度统计
    code_lengths = [len(code) for code in codes]
    print(f"\nCode Length Statistics:")
    print(f"Average code length: {np.mean(code_lengths):.2f}")
    print(f"Min code length: {min(code_lengths)}")
    print(f"Max code length: {max(code_lengths)}")
    
    # 打印每个簇的大小分布
    print("\nCluster Size Distribution:")
    size_distribution = {}
    for cluster in valid_clusters:
        size = cluster["size"]
        if size in size_distribution:
            size_distribution[size] += 1
        else:
            size_distribution[size] = 1
    
    for size in sorted(size_distribution.keys()):
        print(f"Size {size}: {size_distribution[size]} clusters")
    
    return valid_clusters

def filter_suspicious_samples(codes, file_paths):
    """过滤掉可疑的代码样本"""
    filtered_codes = []
    filtered_paths = []
    filtered_stats = {
        'total': len(codes),
        'long_encoded': 0,     # 长编码/混淆代码
        'very_long_code': 0,   # 异常长的代码
        'total_filtered': 0
    }
    
    for code, path in zip(codes, file_paths):
        should_filter = False
        encoding_ratio = extract_code_features(code)
        
        # 只检查编码内容的长度，不进行解码
        encoded_length = int(len(code) * encoding_ratio)
        if encoded_length > 8000:  # 如果编码/混淆内容超过8000字符
            filtered_stats['long_encoded'] += 1
            should_filter = True
            
        # 检查代码总度
        if len(code) > 20000:  # 如果代码长度超过2万字符
            filtered_stats['very_long_code'] += 1
            should_filter = True
            
        if not should_filter:
            filtered_codes.append(code)
            filtered_paths.append(path)
    
    filtered_stats['total_filtered'] = filtered_stats['total'] - len(filtered_codes)
    
    print("\nFiltering Statistics:")
    print(f"Total samples: {filtered_stats['total']}")
    print(f"Filtered due to long encoded content (>8000 chars): {filtered_stats['long_encoded']}")
    print(f"Filtered due to very long code (>100K chars): {filtered_stats['very_long_code']}")
    print(f"Total samples filtered: {filtered_stats['total_filtered']}")
    print(f"Remaining samples: {len(filtered_codes)}")
    
    return filtered_codes, filtered_paths, filtered_stats

if __name__ == "__main__":
    base_path = "/path/to/malicious/packages"
    output_path = "/path/to/output"
    
    os.makedirs(output_path, exist_ok=True)
    
    # 处理和编码
    X, file_paths, codes = process_and_encode(
        base_path,
        output_path,
        batch_size=256
    )
    
    # 过滤可疑样本
    filtered_codes, filtered_paths, filter_stats = filter_suspicious_samples(codes, file_paths)
    
    # 重新编码过滤后的样本
    print("\nEncoding filtered samples...")
    tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
    model = AutoModel.from_pretrained("microsoft/codebert-base")
    
    filtered_encodings = []
    for code in tqdm(filtered_codes):
        encoding = attention_based_encoding_segmented(code, tokenizer, model)
        if encoding is not None:
            filtered_encodings.append(encoding)
    
    X_filtered = np.vstack(filtered_encodings)
    
    # 自动找到最优的簇数并聚类
    cluster_summary = cluster_and_save(
        X_filtered,
        filtered_paths,
        filtered_codes,
        output_path,
        n_clusters=200
    )
    
    print("\n=== Clustering Analysis Complete ===")
    print(f"Results saved to: {output_path}")
