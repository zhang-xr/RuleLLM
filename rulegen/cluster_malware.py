import os
import json
from pathlib import Path
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import normalize
import torch
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_samples, silhouette_score
from datetime import datetime
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist
from scipy.spatial.distance import pdist
from sklearn.decomposition import PCA
import re


def load_malicious_code(base_path):
    """Load code from packages in the specified directory, concatenating all .py files within each package"""
    package_codes = []
    package_paths = []
    
    packages = [d for d in Path(base_path).iterdir() if d.is_dir()]
    
    for package_path in packages:
        package_code = []
        
        for py_file in package_path.rglob("*.py"):
            if py_file.name.startswith('._') or not py_file.is_file():
                continue
                
            try:
                for encoding in ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']:
                    try:
                        with open(py_file, 'r', encoding=encoding) as f:
                            code = f.read()
                            if code.strip() and isinstance(code, str):
                                package_code.append(code)
                                break
                    except UnicodeDecodeError:
                        continue
            except Exception as e:
                continue
        
        if package_code:
            full_code = "\n\n".join(package_code)
            package_codes.append(full_code)
            package_paths.append(str(package_path))
    
    return package_codes, package_paths

def split_code_into_segments(code, max_length=512):
    lines = code.split('\n')
    segments = []
    current_segment = []

    for line in lines:
        current_segment.append(line)
        if len(' '.join(current_segment).split()) > max_length:
            segments.append('\n'.join(current_segment[:-1]))
            current_segment = [current_segment[-1]]

    if current_segment:
        segments.append('\n'.join(current_segment))

    return segments

def attention_based_encoding(code, tokenizer, model, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """Encode code using CodeBERT"""
    try:
        model = model.to(device)
        inputs = tokenizer(code, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            result = outputs.last_hidden_state[:, 0, :]
            return result.cpu().numpy().squeeze()
    except Exception as e:
        return None

def extract_code_features(code):
    """Extract basic code features for filtering"""
    suspicious_patterns = {
        'hex': r'\\x[0-9a-fA-F]{2}',
        'base64': r'[A-Za-z0-9+/=]{20,}',
        'unicode': r'\\u[0-9a-fA-F]{4}',
    }
    
    total_chars = len(code)
    encoded_chars = 0
    
    for pattern in suspicious_patterns.values():
        matches = re.findall(pattern, code)
        encoded_chars += sum(len(m) for m in matches)
    
    return encoded_chars / total_chars if total_chars > 0 else 0

def attention_based_encoding_segmented(code, tokenizer, model, device='cuda' if torch.cuda.is_available() else 'cpu'):
    segments = split_code_into_segments(code)
    encoded_segments = []
    
    model = model.to(device)
    
    for segment in segments:
        encoding = attention_based_encoding(segment, tokenizer, model, device)
        if encoding is not None:
            encoded_segments.append(encoding)
    
    if encoded_segments:
        return np.mean(encoded_segments, axis=0)
    
    return None

def calculate_cluster_similarity(X_scaled, cluster_labels, cluster_id):
    cluster_samples = X_scaled[cluster_labels == cluster_id]
    if len(cluster_samples) < 2:
        return None
    
    distances = pdist(cluster_samples, metric='euclidean')
    similarity = 1 / (1 + np.mean(distances))
    
    return float(similarity)

def process_and_encode(base_path, output_path, batch_size=32):
    """Process and encode code"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
    model = AutoModel.from_pretrained("microsoft/codebert-base").to(device)
    
    codes, file_paths = load_malicious_code(base_path)
    
    encodings = []
    valid_indices = []
    
    for i in tqdm(range(0, len(codes), batch_size)):
        batch_codes = codes[i:i+batch_size]
        batch_encodings = []
        
        for code in batch_codes:
            encoding = attention_based_encoding_segmented(code, tokenizer, model, device)
            if encoding is not None:
                batch_encodings.append(encoding)
                valid_indices.append(i + len(batch_encodings) - 1)
        
        if batch_encodings:
            encodings.extend(batch_encodings)
            
        if i % 1000 == 0 and i > 0:
            temp_output = {
                "processed_count": len(encodings),
                "valid_indices": valid_indices,
                "timestamp": datetime.now().isoformat()
            }
            with open(Path(output_path) / "encoding_progress.json", 'w') as f:
                json.dump(temp_output, f)
    
    return np.vstack(encodings), [file_paths[i] for i in valid_indices], [codes[i] for i in valid_indices]

def find_optimal_clusters(X, max_clusters=500, step=50):
    X_scaled = normalize(X)
    distortions = []
    silhouette_scores = []
    K = range(50, max_clusters, step)
    
    for k in tqdm(K):
        kmeans = KMeans(n_clusters=k, random_state=42).fit(X_scaled)
        distortions.append(sum(np.min(cdist(X_scaled, kmeans.cluster_centers_, 'cosine'), axis=1)) / X_scaled.shape[0])
        if k > 1:
            silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_, metric='cosine'))
    
    diffs = np.diff(distortions)
    elbow_idx = np.argmax(diffs) + 1
    optimal_k = K[elbow_idx]
    
    best_silhouette_idx = np.argmax(silhouette_scores)
    best_silhouette_k = K[best_silhouette_idx]
    
    return int((optimal_k + best_silhouette_k) / 2)

def recluster_large_clusters(X_pca, clusters, threshold=50):
    cluster_sizes = np.bincount(clusters)
    large_clusters = np.where(cluster_sizes > threshold)[0]
    
    if not large_clusters.any():
        return clusters
        
    new_clusters = clusters.copy()
    max_cluster_id = max(clusters)
    
    for large_cluster in large_clusters:
        indices = np.where(clusters == large_cluster)[0]
        samples = X_pca[indices]
        
        n_subclusters = max(2, len(indices) // 20)
        
        sub_kmeans = KMeans(n_clusters=n_subclusters, random_state=42)
        sub_clusters = sub_kmeans.fit_predict(samples)
        
        for i, idx in enumerate(indices):
            new_clusters[idx] = max_cluster_id + 1 + sub_clusters[i]
        max_cluster_id = max(new_clusters)
    
    return new_clusters

def cluster_and_save(X, file_paths, codes, output_path, n_clusters=400):
    X_scaled = normalize(X)
    pca = PCA(n_components=0.95)
    X_pca = pca.fit_transform(X_scaled)
    
    initial_kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = initial_kmeans.fit_predict(X_pca)
    
    clusters = recluster_large_clusters(X_pca, clusters, threshold=50)
    
    final_n_clusters = len(np.unique(clusters))
    
    final_kmeans = KMeans(n_clusters=final_n_clusters, random_state=42)
    clusters = final_kmeans.fit_predict(X_pca)
    
    cluster_scores = silhouette_samples(X_pca, clusters, metric='cosine')
    
    valid_clusters = []
    similarities = []
    
    for cluster_id in tqdm(range(final_n_clusters)):
        cluster_indices = np.where(clusters == cluster_id)[0]
        
        center_distances = cdist([final_kmeans.cluster_centers_[cluster_id]], X_pca[cluster_indices], metric='cosine')[0]
        
        cluster_members = []
        for idx, dist in zip(cluster_indices, center_distances):
            member = {
                "file_path": file_paths[idx],
                "code": codes[idx],
                "distance_to_center": float(dist),
                "silhouette_score": float(cluster_scores[idx]),
                "code_length": len(codes[idx])
            }
            cluster_members.append(member)
        
        if len(cluster_indices) > 1:
            pair_similarities = cosine_similarity(X_pca[cluster_indices])
            sim_values = pair_similarities[np.triu_indices(len(pair_similarities), k=1)]
            cluster_similarity = float(np.mean(sim_values))
            similarities.append(cluster_similarity)
        else:
            cluster_similarity = "N/A"
            sim_values = []
        
        cluster_info = {
            "cluster_id": int(cluster_id),
            "size": len(cluster_members),
            "average_similarity": cluster_similarity,
            "average_silhouette": float(np.mean([m["silhouette_score"] for m in cluster_members])),
            "all_members": cluster_members,
            "statistics": {
                "min_similarity": float(np.min(sim_values)) if len(sim_values) > 0 else "N/A",
                "max_similarity": float(np.max(sim_values)) if len(sim_values) > 0 else "N/A",
                "average_code_length": float(np.mean([m["code_length"] for m in cluster_members]))
            }
        }
        
        valid_clusters.append(cluster_info)
    
    output_file = Path(output_path) / "malware_clusters1.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(valid_clusters, f, indent=2, ensure_ascii=False)
    
    print(f"\n聚类分析完成:")
    print(f"总簇数: {len(valid_clusters)}")
    if similarities:
        print(f"平均相似度: {np.mean(similarities):.3f}")
    
    return valid_clusters

def filter_suspicious_samples(codes, file_paths):
    """Filter out suspicious code samples"""
    filtered_codes = []
    filtered_paths = []
    filtered_stats = {
        'total': len(codes),
        'long_encoded': 0,
        'very_long_code': 0,
        'total_filtered': 0
    }
    
    for code, path in zip(codes, file_paths):
        should_filter = False
        encoding_ratio = extract_code_features(code)
        
        encoded_length = int(len(code) * encoding_ratio)
        if encoded_length > 8000:
            filtered_stats['long_encoded'] += 1
            should_filter = True
            
        if len(code) > 20000:
            filtered_stats['very_long_code'] += 1
            should_filter = True
            
        if not should_filter:
            filtered_codes.append(code)
            filtered_paths.append(path)
    
    filtered_stats['total_filtered'] = filtered_stats['total'] - len(filtered_codes)
    
    return filtered_codes, filtered_paths, filtered_stats

if __name__ == "__main__":
    base_path = "/path/to/malicious/packages"
    output_path = "/path/to/output"
    
    os.makedirs(output_path, exist_ok=True)
    
    X, file_paths, codes = process_and_encode(
        base_path,
        output_path,
        batch_size=256
    )
    
    filtered_codes, filtered_paths, filter_stats = filter_suspicious_samples(codes, file_paths)
    
    tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
    model = AutoModel.from_pretrained("microsoft/codebert-base")
    
    filtered_encodings = []
    for code in tqdm(filtered_codes):
        encoding = attention_based_encoding_segmented(code, tokenizer, model)
        if encoding is not None:
            filtered_encodings.append(encoding)
    
    X_filtered = np.vstack(filtered_encodings)
    
    cluster_summary = cluster_and_save(
        X_filtered,
        filtered_paths,
        filtered_codes,
        output_path,
        n_clusters=200
    )
