import re
import os
import argparse
import json
import torch
from pathlib import Path
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import cdist
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_samples

def load_code_packages(base_path):
    codes, paths = [], []
    for pkg in Path(base_path).iterdir():
        if pkg.is_dir():
            code_parts = []
            for file in pkg.rglob("*.py"):
                if file.name.startswith('._') or not file.is_file():
                    continue
                for enc in ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']:
                    try:
                        with open(file, 'r', encoding=enc) as f:
                            code = f.read()
                            if code.strip():
                                code_parts.append(code)
                                break
                    except UnicodeDecodeError:
                        continue
            if code_parts:
                codes.append("\n\n".join(code_parts))
                paths.append(str(pkg))
    return codes, paths

def split_code(code, max_len=512):
    lines = code.split('\n')
    segments, segment = [], []
    for line in lines:
        segment.append(line)
        if len(' '.join(segment).split()) > max_len:
            segments.append('\n'.join(segment[:-1]))
            segment = [segment[-1]]
    if segment:
        segments.append('\n'.join(segment))
    return segments

def encode_segmented(code, tokenizer, model, device):
    segments = split_code(code)
    embeddings = []
    for segment in segments:
        inputs = tokenizer(segment, return_tensors="pt", truncation=True, max_length=512).to(device)
        with torch.no_grad():
            output = model(**inputs).last_hidden_state[:, 0, :]
        embeddings.append(output.cpu().numpy())
    if embeddings:
        return np.mean(np.vstack(embeddings), axis=0)
    return None

def extract_encoded_ratio(code):
    patterns = [r'\\x[0-9a-fA-F]{2}', r'[A-Za-z0-9+/=]{20,}', r'\\u[0-9a-fA-F]{4}']
    total = len(code)
    encoded = sum(len(m) for p in patterns for m in re.findall(p, code))
    return encoded / total if total > 0 else 0

def filter_codes(codes, paths):
    clean_codes, clean_paths = [], []
    for code, path in zip(codes, paths):
        if extract_encoded_ratio(code) * len(code) > 8000 or len(code) > 20000:
            continue
        clean_codes.append(code)
        clean_paths.append(path)
    return clean_codes, clean_paths

def cluster_codes(embeddings, paths, codes, output_path, n_clusters=200):
    X_scaled = normalize(embeddings)
    pca = PCA(n_components=0.95)
    X_pca = pca.fit_transform(X_scaled)

    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(X_pca)
    
    silhouette_vals = silhouette_samples(X_pca, labels)

    cluster_similarities = {}
    all_sims = cosine_similarity(X_pca)
    global_sim = np.mean(all_sims[np.triu_indices(len(X_pca), k=1)])
    
    valid_clusters = []

    print("Processing clusters...")
    for cluster_id in range(n_clusters):
        cluster_indices = np.where(labels == cluster_id)[0]
        
        center_distances = cdist([kmeans.cluster_centers_[cluster_id]], X_pca[cluster_indices], metric='cosine')[0]
        
        cluster_members = []
        for idx, dist in zip(cluster_indices, center_distances):
            member = {
                "file_path": paths[idx],
                "code": codes[idx],
            }
            cluster_members.append(member)
        
        if len(cluster_indices) > 1: 
            pair_similarities = cosine_similarity(X_pca[cluster_indices])
            sim_values = pair_similarities[np.triu_indices(len(pair_similarities), k=1)]
            cluster_similarity = float(np.mean(sim_values))
            cluster_similarities[cluster_id] = cluster_similarity
        else:
            cluster_similarity = 0.0
            sim_values = []
        
        cluster_info = {
            "cluster_id": int(cluster_id),
            "size": len(cluster_members),
            "all_members": cluster_members
        }
        
        valid_clusters.append(cluster_info)
    
    avg_similarity = np.mean(list(cluster_similarities.values())) if cluster_similarities else 0
    
    print(f"Total number of clusters: {n_clusters}")
    print(f"Average intra-cluster similarity: {avg_similarity:.4f}")
    print(f"Global average similarity: {global_sim:.4f}")
    print(f"Improvement over random: {avg_similarity/global_sim:.2f}x")
    
    output_file = Path(output_path) / "malware_clusters.json"
    with open(output_file, "w") as f:
        json.dump(valid_clusters, f, indent=2)
    
    print(f"Results successfully saved to: {output_file.absolute()}")

def main():
    parser = argparse.ArgumentParser(description="Malicious code clustering")
    parser.add_argument("--base_path", type=str, default="/media/zxr/Elements/malicious-software-packages-dataset/samples/pypi")
    parser.add_argument("--output_path", type=str, default="/home/zxr/code/rulellm/cluster")
    args = parser.parse_args()

    os.makedirs(args.output_path, exist_ok=True)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
    model = AutoModel.from_pretrained("microsoft/codebert-base").to(device)

    codes, paths = load_code_packages(args.base_path)
    codes, paths = filter_codes(codes, paths)

    embeddings = []
    for code in tqdm(codes):
        emb = encode_segmented(code, tokenizer, model, device)
        if emb is not None:
            embeddings.append(emb)

    X = np.vstack(embeddings)
    cluster_codes(X, paths, codes, args.output_path)

if __name__ == "__main__":
    main()
# python your_script.py --base_path /path/to/data --output_path /path/to/save
